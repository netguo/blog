### 云原生发展

自从 2013 年 Docker 诞生以来，容器一跃成为了 IT 界最热门的话题。而 Kubernetes 则趁着容器的“东风”，借助 Google 和 CNCF 的强力“背书”，击败了 Docker Swarm 和 Apache Mesos，成为了“容器编排”领域的王者至尊。
不管你是研发、测试、还是运维，不管你是前台、后台、还是中台，不管你用的是 C++、Java 还是 Python，不管你是搞数据库、区块链、还是人工智能，不管你是从事网站、电商、还是音视频，在这个“云原生”时代，Kubernetes 都是一个绕不过去的产品，是我们工作中迟早要面对的“坎儿”。
![image.png](https://cdn.nlark.com/yuque/0/2023/png/8364057/1676863321470-e29abed1-d5f4-42c8-b232-dd9379684dd3.png#averageHue=%23dfd39e&clientId=u1027a218-f7b9-4&from=paste&height=217&id=mi0gE&originHeight=433&originWidth=1080&originalType=binary&ratio=2&rotation=0&showTitle=false&size=312798&status=done&style=none&taskId=u27571c6e-a01f-4284-b3a8-9ca39ef1cd8&title=&width=540)

####  源起

2013年，Solomon Hykes在一个技术论坛上发表了一个“The future of Linux Containers”主题演讲，也就是现在的Docker，讲述如何通过容器化解决打包、部署、运维等“老大难”问题。之后边引起了许多大厂商的关注，之后云原生技术如雨后春笋，快速发展。

**Docker可以帮我们解决什么问题？**

- 跨机器的绿色部署：Docker提供了一种将应用及其所有环境依赖都打包到一起的格式
- 以应用为中心的封装：Docker封装应用而非机器的理念，正是它崛起的原因
- 自动构建：Docker提供了开发人员在容器中构建产品的全部支持
- 多版本支持：Docker支持像Git一样管理容器的连续版本
- 组建重用：Docker允许将任何现有容器作为基础镜像使用
- 共享：Docker拥有公共镜像仓库
- 工具生态：Docker开放了一套可自动化和自行扩展的接口

**看到上述，是不是觉得Docker真正引领了时代？**
Docker其实并不是一个黑科技式的秘密武器，早在2008年，Linux Kernel 2.6.24就发布了Linux容器（LXC），LXC是一种封装系统的轻量级虚拟机，而Docker是一种封装应用的技术手段。封装应用，更符合当下技术的趋势，因此引爆了一个时代。Docker和LXC都利用了Linux的chroot，cgroup，namespace技术，这三个技术构建了容器化的基础。
**chroot** 
chroot是英文单词“change root”的缩写，功能是当某进程经过chroot操作之后，它的根目录就会被锁定在指定位置，该进程或者它的自进程，不能访问目录之外的文件。
**namespace**
命名空间，是针对程序设计的访问隔离机制，在独立命名空间内，拥有系统的一切资源，不仅仅是文件独立，还有自己独立的进程号、UID/GID编号(例如独立root用户)、网络(例如独立IP、防火墙等)等等。
**cgroup**
如果让一台物理机中各进程看起来像独享，不仅仅需要隔离资源，还必须能独立控制各个进程的资源使用配额。cgroup的功能就是如此，用于隔离或者分配并限制某个进程组能够使用的资源配额，例如内存，CPU处理时间，磁盘I/O速度等。

#### Kubernetes

如果说以Docker为代表的容器引擎是软件的发布流程从分发安装包到分发虚拟运行环境，使得应用得以实现跨机器绿色部署。那么以K8S为代表的容器编排就是把大型软件系统运行所依赖的集群环境进行了虚拟化，另集群得以实现跨数据中心的绿色部署，并能够根据实际情况进行扩容。
K8S源于Google内部已经运行多年的集群管理系统Borg，用go语言完全重写后，2014年6月开源。从诞生之日就收到各大厂商追捧，多年的积累使得与Docker Swarm的容器编排战争胜利是必然的。
尽管早在2013年就提出了“云原生”的概念，但是要实现服务化、具备韧性、弹性、可观测性的软件系统十分困难，在当时只能依靠程序员高超的个人能力，云计算本身帮不上什么忙。直到Kubernetes出世，大家才等到了破局的希望，认准了这就是云原生时代的操作系统。

**K8S与Docker的恩怨情仇**

![image.png](https://cdn.nlark.com/yuque/0/2023/png/8364057/1676878583794-0ad69e25-919e-42f4-9bb9-2df46d37668f.png#averageHue=%23f1ece0&clientId=u81256c50-fb01-4&from=paste&height=136&id=ud6a36501&originHeight=136&originWidth=572&originalType=binary&ratio=2&rotation=0&showTitle=false&size=27593&status=done&style=none&taskId=u17d7df4a-98ca-4536-8bec-f4fdcb0ffcc&title=&width=572)
2014年Docker已经如日中天，K8S才刚刚诞生，自然就选择了在Docker上运行，到2016年，K8S已经快速成长，并加入CNCF(云原生基金会)，成为CNCF第一个托管项目。在2016年底，K8S在1.5版本引入一个新的接口标准CRI（Container Runtime Interface），规定了kubelte如何去调度容器，这与之前Docker调用完全不兼容。意思很明显，可以支持其它容器。不过当时Docker已经非常强大，提供了一个“适配器”CRI shim用于兼容Docker接口。
Docker由于公司体量太小，无法与K8S抗衡，把原本单体架构Docker Engine，拆分成多个模块，把符合CRI的Docker daemon部分捐赠给了CNCF，如今便形成了第二调用链路。
![image.png](https://cdn.nlark.com/yuque/0/2023/png/8364057/1676879345183-2fa877cb-58a1-420d-b084-ebda49e6546b.png#averageHue=%23f8f8f8&clientId=u81256c50-fb01-4&from=paste&height=627&id=u3acc144e&originHeight=627&originWidth=1920&originalType=binary&ratio=2&rotation=0&showTitle=false&size=159287&status=done&style=none&taskId=u23c3cf42-9517-41d3-9f8e-1b3b386b69e&title=&width=1920)
2020年，K8S1.2宣布弃用Docker的支持（dockershim），并会在未来版本中彻底删除。

**Docker的未来**

虽然Docker在容器编排之争败给K8S，K8S可以支持多种容器，并弃用Docker的支持。但是但从容器角度，Docker积累了大量的客户，开发简便，技术成熟，还是难以被替代的。

### Kubernetes技术简介

#### 概念

Kubernetes 为你提供了一个可弹性运行分布式系统的框架。 Kubernetes 会满足你的扩展要求、故障转移你的应用、提供部署模式等。
容器类似于 VM，但是更宽松的隔离特性，使容器之间可以共享操作系统（OS）。 因此，容器比起 VM 被认为是更轻量级的。且与 VM 类似，每个容器都具有自己的文件系统、CPU、内存、进程空间等。 由于它们与基础架构分离，因此可以跨云和 OS 发行版本进行移植。
Kubernetes 为你提供：

- 服务发现和负载均衡
- 存储编排
- 自动部署和回滚
- 自动完成装箱计算
- 自我修复
- 密钥与配置管理

Kubernetes 为构建开发人员平台提供了基础，但是在重要的地方保留了用户选择权，能有更高的灵活性。Kubernetes不是什么：

- 不限制支持的应用程序类型。 
- 不部署源代码，也不构建你的应用程序。
- 不提供应用程序级别的服务作为内置服务，例如中间件、 数据处理框架、数据库等。
- 不是日志记录、监视或警报的解决方案。 
- 不提供也不要求配置用的语言、系统。
- 不提供也不采用任何全面的机器配置、维护、管理或自我修复系统。
- 此外，Kubernetes 不仅仅是一个编排系统，实际上它消除了编排的需要。

#### Kubernetes架构

![image.png](https://cdn.nlark.com/yuque/0/2023/png/8364057/1676881063315-286497e2-60ae-4ca8-ac8a-8f5becd94a45.png#averageHue=%23fbf5ef&clientId=u3fd580c8-1c55-4&from=paste&height=704&id=u5e82ca7a&originHeight=704&originWidth=1278&originalType=binary&ratio=1&rotation=0&showTitle=false&size=183140&status=done&style=none&taskId=ua1a4ca70-868a-4e70-9c77-166de4228c1&title=&width=1278)
Kubernetes 采用了现今流行的“控制面/数据面”（Control Plane / Data Plane）架构，集群里的计算机被称为“节点”（Node），可以是实机也可以是虚机，少量的节点用作控制面来执行集群的管理维护工作，其他的大部分节点都被划归数据面，用来跑业务应用。
控制面的节点在 Kubernetes 里叫做 Master Node，数据面的节点叫做 Worker Node。Node 的数量非常多，构成了一个资源池，Kubernetes 就在这个池里分配资源，调度应用。因为资源被“池化”了，所以管理也就变得比较简单，可以在集群中任意添加或者删除节点。

**Master组件**

- apiserver：是 Master 节点——同时也是整个 Kubernetes 系统的唯一入口，它对外公开了一系列的 RESTful API，并且加上了验证、授权等功能，所有其他组件都只能和它直接通信。
- etcd：是一个高可用的分布式 Key-Value 数据库，用来持久化存储系统里的各种资源对象和状态。
- scheduler：负责容器的编排工作，检查节点的资源状态，把 Pod 调度到最适合的节点上运行。
- controller-manager 负责维护容器和节点等资源的状态，实现故障检测、服务迁移、应用伸缩等功能。

**Node组件**

- kubelet：Node的代理，负责管理Node 相关的绝大部分操作，Node 上只有它能够与 apiserver 通信，实现状态报告、命令下发、启停容器等功能。
- kube-proxy：的作用有点特别，它是 Node 的网络代理，只负责管理容器的网络通信，简单来说就是为 Pod 转发 TCP/UDP 数据包。
- ontainer-runtime：它是容器和镜像的实际使用者，在 kubelet 的指挥下创建容器，管理 Pod 的生命周期。

**插件**

只要服务器节点上运行了 apiserver、scheduler、kubelet、kube-proxy、container-runtime 等组件，就可以说是一个功能齐全的 Kubernetes 集群了。不过就像 Linux 一样，操作系统提供的基础功能虽然“可用”，但想达到“好用”的程度，还是要再安装一些附加功能，这在 Kubernetes 里就是插件（Addon）。例如：DNS、Dashboard等。

#### Kubernetes对象

在 Kubernetes 系统中，Kubernetes 对象 是持久化的实体。 Kubernetes 使用这些实体去表示整个集群的状态。 比较特别地是，它们描述了如下信息：

- 哪些容器化应用正在运行（以及在哪些节点上运行）
- 可以被应用使用的资源
- 关于应用运行时表现的策略，比如重启策略、升级策略以及容错策略

Kubernetes 对象是“目标性记录” —— 一旦创建该对象，Kubernetes 系统将不断工作以确保该对象存在。
管理对象，有指令式命令、指令式对象配置、声明式对象配置。生产系统我们通常用指令式对象配置和声明式对象配置，具体yaml格式和配置可以参考官网。
![image.png](https://cdn.nlark.com/yuque/0/2023/png/8364057/1676965377827-ef2bc4a2-6463-4e50-9493-ab3e7ef7a96c.png#averageHue=%23181715&clientId=u3fd580c8-1c55-4&from=paste&height=564&id=u4af45eb5&originHeight=564&originWidth=1170&originalType=binary&ratio=1&rotation=0&showTitle=false&size=142507&status=done&style=none&taskId=ufc719601-1df1-416d-8f1c-e024556daef&title=&width=1170)
可以通过kubectl api-resources查看，可以看到日常我们使用的一切资源，都是对象，类似Linux一切都是文件设计，一切皆是资源。

#### 资源调度

**资源模型**

资源是什么？广义上讲Kubernetes系统所有能接触的方方面面都被抽象成了资源，例如工作负荷资源（Pod、ReplicaSet），存储资源（Volume、PersistentVolume等），表示策略的资源（LimitRange、Resource Quota）等等。Kubernetes以资源为载体，建立一套同时囊括抽象元素和物理元素的特定语言。通过不同层级间资源的使用关系来描述上至集群，下至一块内存区域。对资源的描述集合共同构成了一幅全景图。此处我们讨论的是物理资源。
从编排角度，Node是资源的提供者，Pod是资源的使用者，调度是对两者进行恰当的撮合。Node通常能够提供三方面资源：计算资源（CPU、内存、GPU）、存储资源、网络资源。CPU资源是可被压缩资源，特点资源不足，Pod只会运行变慢，不会被系统杀死。如果内存溢出，会被系统杀死。
Kubernetes给处理器资源设定的默认计量单位是“逻辑处理器个数”，对于内存来说已经有它广泛使用的计量单位。

**服务质量与优先级**

Kubernetes给出的资源配额有requests和limits两项设计。这个是谷歌根据“大多数用户提交资源配置时，远远大于实际使用资源”经验来设计的，K8S会根据requests的值进行决策选择哪个节点运行Pod，向cgroups传递资源配额时，会按照limits的值来进行设置。这样的设计在极端情况下，节点给Pod分配的资源如果超过自己最大可提供的资源，且这些Pod的总消耗真的超标，K8S只能停止一部分Pod保证其它Pod的正常运行，牺牲哪些Pod的准则就是K8S的服务质量等级与优先级的概念。
服务质量等级一共分为三级由高到低分别时Guaranteed、Burstable、BestEffort。如果Pod中所有的容器都设置了limits和requests，且两者的值相等，那此Pod的服务质量等级便为最高的Guaranteed；如果Pod中有部分容器的requests值小于limits值，或者只设置了requests而未设置limits，那此Pod的服务质量等级为第二 Burstable；如果是imits和requests两个都没设置则属于最低的BestEffort。
优先级决定了Pod之间并不是平等的关系，且这种不平等不是谁会占用更多资源的问题，而是会直接影响Pod调度与生存的关键。调度时，高优先级的Pod会优先被调度。但受优先级影响更大的另一方面是指Kubernetes的抢占机制（Preemption），在正常未设置优先级的情况下，如果Pod调度失败，就会暂时处于Pending状态被搁置起来，直到集群中有新节点加入或者旧Pod退出。但是，如果有一个被设置了明确优先级的Pod调度失败无法创建的话，Kubernetes就会在系统中寻找一批牺牲者（Victim），将它们杀掉以便给更高优先级的Pod让出资源。

**驱逐机制**

阈值设置，不可压缩资源内存，磁盘、文件可用inode数量，可用的容器运行时镜像空间可用数值、百分比来设置，例如磁盘可用率<10%，内存<100M。
驱逐Pod时一种破坏性行为，有可能导致服务中断，必须更加谨慎。因此就有了软驱逐、硬驱逐、优雅退出的概念。

-  软驱逐：通常配置一个较低的警戒线（譬如可用内存仅剩20%），触及此线时，系统将进入一段观察期。如果只是暂时的资源抖动，在观察期内能够恢复到正常水平的话，那就不会真正启动驱逐操作。否则，若资源持续超过警戒线一段时间，就会触发Pod的优雅退出。
-  硬驱逐：通常配置一个较高的终止线（譬如可用内存仅剩10%），一旦触及此红线，立即强制杀掉Pod，而不会优雅退出。

还有个问题，Kubernetes中很少会单独创建Pod，通常都是由ReplicaSet、Deployment等更高层资源来管理，这意味着当Pod被驱逐之后，它不会从此彻底消失，Kubernetes将自动生成一个新的Pod来取代，并经过调度选择一个节点继续运行。。如果没有额外的处理，那很大概率这个Pod会被系统调度到当前节点上重新创建。为了避免上述情况，Kubernetes还提供了设置在驱逐发生之后多长时间内不得往该节点调度Pod。

**默认调度器**

Kubernetes是如何撮合Pod与Node的？在几个、十几个节点的集群里进行调度，调度器怎么实现都不会太困难，但是对于数千个乃至更多节点的大规模集群，要实现高效的调度就绝不简单。
![image.png](https://cdn.nlark.com/yuque/0/2023/png/8364057/1676975193535-76fb158e-8653-4509-af0b-352b2542d914.png#averageHue=%23fdfdfd&clientId=u3fd580c8-1c55-4&from=paste&height=264&id=uc0763978&originHeight=636&originWidth=1500&originalType=binary&ratio=1&rotation=0&showTitle=false&size=182638&status=done&style=none&taskId=u4463200e-929e-4c86-94b3-033095415c9&title=&width=623)
默认实现，是如上图所示，状态共享的双循环。
第一个循环
Informer持续监视etcd中与调度相关资源变化，Pod、Node等资源出现变动时，会触发对应Informer的Handler，更新调度队列和调度缓存中的信息。如有必要，还会根据优先级提前插队和抢占。
第二个循环
不停的将调度队列中Pod出队，然后使用Predicate算法进行节点选择。Predicate本质上是一组节点过滤器，默认有三种过滤器：
通用过滤器：检查节点是否满足Pod中需要的资源需求。
卷过滤策略：与存储相关的过滤策略，用来检查节点挂载的Volume是否存在冲突。
节点过滤策略：与宿主机相关的过滤策略，最典型的是Kubernetes的污点和容忍度。
Predicate选择节点，会对节点进行打分，K8S提供一系列相关算法。选出合适的节点。
在提交之前，需要重新确定，重新调度Predicate算法，调度成功后，通知目标节点的kubelet去创建节点，如果kubelet检查到节点有变化，还会重新执行一次确认工作，创建过程采用乐观绑定，如果失败，回滚状态，把Pod重新加入调度缓存。

#### 存储

Kubernetes在规划持久化存储能力的时候，依然遵循着它的一贯设计哲学，即用户负责以资源和声明式API来描述自己的意图，Kubernetes负责根据用户意图来完成具体的操作。
Kubernetes引入存储卷Volume概念，分为持久化的PersistentVolume和非持久化的普通Volume两类。

**静态存储**

普通Volume的设计目标不是为了持久地保存数据，而是为同一个Pod中多个容器提供可共享的存储资源，因此Volume具有十分明确的生命周期——与挂载它的Pod相同的生命周期。
只有开发人员能准确评估Pod需要消耗多大的存储空间，只有运维人员清楚地知道当前系统可以使用的存储设备状况。为了让他们得以提供各自擅长的信息，Kubernetes又额外设计出了PersistentVolumeClaim资源。PV来创建资源，Pod通过挂载PVC去申请资源。
具体实现参考下图：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/8364057/1677052582470-beb1fca8-ebbc-4f55-be6b-dffc9796b476.png#averageHue=%23f6f1e1&clientId=u50cdf4a8-e48c-4&from=paste&height=1310&id=ua36b1655&originHeight=1310&originWidth=1920&originalType=binary&ratio=2&rotation=0&showTitle=false&size=931997&status=done&style=none&taskId=u38dae93b-9abb-497c-ad28-ee6aa9d7d72&title=&width=1920)

**动态存储**

动态存储分配方案，是指在用户声明存储能力的需求时，不是通过Kubernetes撮合来获得一个管理员人工预置的PersistentVolume，而是由特定的资源分配器（Provisioner）自动地在存储资源池或者云存储系统中分配符合用户存储需求的PersistentVolume，然后挂载到Pod中使用。
具体实现参考下图，分别为本地文件存储和外部存储系统存储：

![image.png](https://cdn.nlark.com/yuque/0/2023/png/8364057/1677051450974-d329a6c8-52b9-4eb0-b678-db28dee6ec09.png#averageHue=%23f7f4e7&clientId=u50cdf4a8-e48c-4&from=paste&height=1125&id=uc103e694&originHeight=1125&originWidth=1920&originalType=binary&ratio=2&rotation=0&showTitle=false&size=782054&status=done&style=none&taskId=u9a90e21f-d981-4f16-8186-e337e26929a&title=&width=1920)

**容器存储插件**

现在几乎所有云计算厂商都支持自家的容器通过CSI规范去接入外部存储，存储插件多达上百款。目前出现过的存储系统和设备均可以划分到块存储、文件存储和对象存储这三种存储类型之中。

#### 网络

容器之间的通信可以归结为本地主机内部的多容器之间、本地主机与内部容器之间、跨越不同主机的多个容器之间的通信问题。
网络的专业性和针对性是极强的，厂商自己做是吃力不讨好的，是以插件的形式提供的，网络厂商可以通过插件实现网络管理和IP地址管理。时至今日，支持CNI的网络插件已多达数十种，跨主机通信的网络实现模式只有下面三种：

**overlay模式**

这是一种虚拟化的上层逻辑网络，好处在于它不受底层物理网络结构的约束，有更大的自由度，更好的易用性；坏处是由于额外的包头封装导致信息密度降低，额外的隧道封包、解包会导致传输性能下降。
常见的插件有：Flannel（VXLAN模式）、Calico（IPIP模式）、Weave等。

**路由模式**

路由模式其实属于Underlay模式的一种特例，这里将它单独作为一种网络实现模式来介绍。相比Overlay网络，路由模式的主要区别在于它的跨主机通信是直接通过路由转发来实现的，因而无须在不同主机之间进行隧道封包。
这种模式的好处是性能比Overlay网络有明显提升，坏处是路由转发要依赖底层网络环境的支持，并不是你想做就能做到的。路由网络要求要么所有主机都位于同一个子网之内，都是二层连通的，要么不同二层子网之间由支持边界网关协议（Border Gateway Protocol，BGP）的路由相连，并且网络插件也同样支持BGP协议去修改路由表。

**Underlay模式**

这里的Underlay模式特指让容器和宿主机处于同一网络，两者拥有相同地位的网络方案。Underlay网络要求容器的网络接口能够直接与底层网络进行通信，因此该模式是直接依赖于虚拟化设备与底层网络能力的。
对于真正的大型数据中心、大型系统，Underlay模式才是最有发展潜力的网络实现模式。这种模式能够最大限度地利用硬件的能力，往往有着最优秀的性能表现。但也是由于它直接依赖于硬件与底层网络环境，必须根据软、硬件情况来进行部署，难以做到Overlay网络那样开箱即用的灵活性。
常见的Underlay网络插件有MACVLAN、SR-IOV（Single Root I/O Virtualization）等。

### 参考
《凤凰架构》
《K8S入门实战》

